{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiles construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cytomine and SLDC modules need to be installed\n",
    "\n",
    "https://doc.cytomine.com/admin-guide/ce/ce-install\n",
    "\n",
    "https://github.com/waliens/sldc-cytomine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sldc_cytomine\n",
    "import sldc\n",
    "from sldc import Segmenter, DispatchingRule, PolygonClassifier, report_timing, StandardOutputLogger, Logger\n",
    "from sldc.builder import SLDCWorkflowBuilder\n",
    "from sldc.image import TileTopology, FixedSizeTileTopology\n",
    "from sldc_cytomine.tile_builder import CytomineTileBuilder\n",
    "from sldc_cytomine.tile import CytomineTile\n",
    "from sldc_cytomine.dump import _infer_image_region, _image_from_zone\n",
    "from sldc_cytomine.autodetect import infer_protocols\n",
    "from cytomine import Cytomine\n",
    "from cytomine.models import PropertyCollection, Property, CurrentUser, Project,ImageInstance, Annotation, Term, ProjectCollection,ImageInstanceCollection, AnnotationCollection, TermCollection, Property, AnnotationTerm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connexion to Cytomin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "host = 'http://cytomine.ulb.ovh'\n",
    "public_key = 'public_key'\n",
    "private_key = 'private_key'\n",
    "\n",
    "\n",
    "\n",
    "with Cytomine(host, public_key, private_key) as cytomine:\n",
    "    me = CurrentUser().fetch()\n",
    "    \n",
    "\n",
    "    projects = ProjectCollection().fetch()\n",
    "    for project in projects:\n",
    "        proj_id = project.id\n",
    "    \n",
    "    phytolithes = Project().fetch(id=proj_id)\n",
    "\n",
    "    images = ImageInstanceCollection().fetch_with_filter(\"project\", proj_id)\n",
    "    for image in images:\n",
    "        print(image.name)\n",
    "        print(image.id)\n",
    "        print(\"Taille de l'image : {} x {}\".format(image.height, image.width))\n",
    "        print(\"Résolution de l'image: {}\".format(image.resolution))\n",
    "        im_id = image.id\n",
    "\n",
    "    image = ImageInstance().fetch(id = im_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction of the tiles of size 256x256 and an overlap of 0. The Fixed Size Tile Topology is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_file = \"tiles\"\n",
    "if not os.path.exists(tiles_file):\n",
    "    os.makedirs(tiles_file)\n",
    "\n",
    "logger = StandardOutputLogger(Logger.INFO)\n",
    "slide_class, tile_class = infer_protocols(_image_from_zone(im_id))\n",
    "im = _infer_image_region(im_id, 0, slide_class)\n",
    "tile_builder = CytomineTileBuilder(working_path=\"Cache 256\", tile_class= tile_class)\n",
    "topology = TileTopology(image = im, tile_builder=tile_builder, max_width= 256, max_height= 256, overlap= 0)\n",
    "topology = FixedSizeTileTopology(topology)\n",
    "iter = topology.__iter__()\n",
    "\n",
    "for t in iter:\n",
    "    coord = t.offset\n",
    "    x = coord[0]\n",
    "    y = coord[1]\n",
    "    t_np = t.np_image\n",
    "    plt.imsave(\"tiles_path/{}_{}_{}_{}_{}.png\".format(im_id, t.offset_x,t.offset_y, t.width, t.height), t_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction of an image tile at a defined position and size for a defined builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tile(builder, coord, height, width):\n",
    "    tile = im.tile(builder, coord, height,width)\n",
    "    print(\"tile constructed\")\n",
    "    tile_np = tile.np_image\n",
    "    return tile_np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import config as tf_config\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_image(image, channels=3)\n",
    "    #Normalization\n",
    "    image = tf.cast(image, tf.float64) / 255.0  \n",
    "    return image\n",
    "\n",
    "def create_dataset(image_paths, annotation_paths):\n",
    "    images = [load_and_preprocess_image(path) for path in image_paths]\n",
    "    annotations = [parse_yolo_annotation(path) for path in annotation_paths]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, annotations))\n",
    "    return dataset\n",
    "\n",
    "def preprocess_data(image, annotation):\n",
    "    # Expand dimensions for the image\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    \n",
    "    # Cast annotation to tf.float32\n",
    "    annotation = tf.cast(annotation, tf.float32)\n",
    "    \n",
    "    return image, annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the annotations and predictions txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_annotations(annotation_path):\n",
    "    with open(annotation_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    annotations = []\n",
    "    for line in lines:\n",
    "        values = line.strip().split()\n",
    "        class_label = int(values[0])\n",
    "        x, y, width, height = map(float, values[1:])\n",
    "        annotations.append((class_label, x, y, width, height))\n",
    "\n",
    "    return annotations\n",
    "\n",
    "def read_predictions(predictions_path):\n",
    "    with open(predictions_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    boxes = []\n",
    "    for line in lines:\n",
    "        data = eval(line)\n",
    "        x_sldc, y_sldc = map(int, data[0:2])\n",
    "        x_c, y_c, w, h = map(float, data[2:])\n",
    "        \n",
    "        boxes.append([x_sldc, y_sldc, x_c, y_c, w, h])\n",
    "\n",
    "    return np.array(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert YOLO format to TensorFlow bounding box format (x_center, y_center, width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_yolo_annotation(annotation_path):\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    boxes = []\n",
    "    for line in lines:\n",
    "        data = line.strip().split()\n",
    "        class_label = int(data[0])\n",
    "        x, y, w, h = map(float, data[1:])\n",
    "        \n",
    "        boxes.append([x, y, w, h])\n",
    "\n",
    "    return np.array(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allows to draw a Bounding Box representing the annotation on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bounding_boxes(image_path, annotations):\n",
    "    image = plt.imread(image_path)\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for annotation in annotations:\n",
    "        print(annotation)\n",
    "        class_label, x, y, width, height = annotation\n",
    "        x *= image.shape[1]\n",
    "        y *= image.shape[0]\n",
    "        width *= image.shape[1]\n",
    "        height *= image.shape[0]\n",
    "        bbox = patches.Rectangle((x - width / 2, y - height / 2), width, height,\n",
    "                                 linewidth=2, edgecolor='g', facecolor='none')\n",
    "        ax.add_patch(bbox)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths_from_folder(folder_path, extension):\n",
    "    file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(extension)]\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of pre-trained VGG16\n",
    "base_model = tf.keras.applications.VGG16(\n",
    "    include_top=False, weights='imagenet', input_shape=(256, 256, 3)\n",
    ")\n",
    "\n",
    "# Fiw the weights\n",
    "base_model.trainable = False\n",
    "\n",
    "# Adding the regression head\n",
    "x = layers.Conv2D(32, 3, activation='relu')(base_model.output)\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.Conv2D(128, 3, activation='relu')(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "predictions = layers.Dense(4)(x)  \n",
    "# Assuming 4 outputs for bounding box coordinates\n",
    "\n",
    "\n",
    "model = models.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Loading of the weights \n",
    "model.load_weights('best.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of the model on the tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_folder = tiles_file\n",
    "\n",
    "\n",
    "# Get a list of all image filenames in the folder\n",
    "image_filenames = [filename for filename in os.listdir(image_folder) if filename.endswith(('.png', '.jpg'))]\n",
    "for image_filename in image_filenames:\n",
    "    image_path = os.path.join(image_folder, image_filename)\n",
    "    \n",
    "    \n",
    "#  Text files creation with [0 0 0 0 0] labels for entry in the model\n",
    "annotations_folder = 'labels'\n",
    "if not os.path.exists(annotations_folder):\n",
    "    os.makedirs(annotations_folder)\n",
    "for image_filename in image_filenames:\n",
    "    path = os.path.join(image_folder, image_filename.replace('.png', '.txt'))\n",
    "    with open(path, 'w') as file:\n",
    "        file.write(\"0 0 0 0 0 \\n\")\n",
    "        file.close()\n",
    "        \n",
    "    \n",
    "image_paths_test = get_file_paths_from_folder(image_folder, extension=(\".png\", \".jpg\"))\n",
    "annotation_paths_test = get_file_paths_from_folder(annotations_folder, extension=\".txt\")\n",
    "\n",
    "predictions_file = 'predictions'\n",
    "if not os.path.exists(predictions_file):\n",
    "    os.makedirs(predictions_file)\n",
    "\n",
    "nbr_waves = math.ceil(len(image_filenames)/1000)\n",
    "\n",
    "\n",
    "for i in range(nbr_waves):\n",
    "    start = (i)*1000\n",
    "    if (start+1000)<= len(image_filenames):\n",
    "        end = start+1000\n",
    "    else:\n",
    "        end = len(image_filenames)\n",
    "        print(\"all predictions done\")\n",
    "\n",
    "    dataset = create_dataset(image_paths_test[start:end], annotation_paths_test[start:end])\n",
    "    dataset = dataset.map(preprocess_data)\n",
    "    predictions = model.predict(dataset)\n",
    "\n",
    "    for j, (image, label) in enumerate(dataset):\n",
    "        name_im = image_filenames[start + j]\n",
    "        infos = name_im.split('_')\n",
    "        infos = [int(number) for number in infos if number.isdigit()]\n",
    "    \n",
    "        coords = [infos[1], infos[2]]\n",
    "        save_path = os.path.join(predictions_file, 'predictions_' + str(i) + '.txt')\n",
    "        \n",
    "        pred_box = predictions[j]  \n",
    "        coords.extend(pred_box.tolist())\n",
    "        with open(save_path , 'a') as file:\n",
    "            file.write(str(coords) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of the detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from skimage.filters import sobel_h, sobel_v\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "# Tenenbaum gradient (not the Wes Anderson movie)\n",
    "def tenenbaum(image):\n",
    "    sob_x = sobel_h(image)\n",
    "    sob_y = sobel_v(image)\n",
    "    sum_sq = sob_x**2 + sob_y**2\n",
    "    ten = np.sum(sum_sq)\n",
    "    return ten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "slide_class, tile_class = infer_protocols(_image_from_zone(im_id))\n",
    "tile_builder = CytomineTileBuilder(working_path=\"Cache 256\", tile_class= tile_class)\n",
    "image_micro = _infer_image_region(im_id, 0, slide_class)\n",
    "\n",
    "tot_saved_tiles = 0\n",
    "tot_blurr_tiles = 0\n",
    "tot_small_area = 0\n",
    "\n",
    "\n",
    "for j in range(len(predictions_file)+1):\n",
    "\n",
    "    # print(\"WAVE\", j)\n",
    "    \n",
    "    predictions_path = os.path.join(predictions_file, 'predictions_'+str(j)+'.txt')\n",
    "    predictions = read_predictions(predictions_path)\n",
    "    \n",
    "    detections_path = 'detections_images'\n",
    "    if not os.path.exists(detections_path): os.makedirs(detections_path)\n",
    "    \n",
    "\n",
    "    count_neg_coord = 0\n",
    "    count_side_zero = 0\n",
    "    count_small_area = 0\n",
    "    count_saved_tiles = 0\n",
    "    count_blur = 0\n",
    "\n",
    "    for i in range (len(predictions)):\n",
    "        \n",
    "        d_test = predictions[i]\n",
    "\n",
    "        x_sldc = d_test[0]\n",
    "        y_sldc = d_test[1]\n",
    "\n",
    "        x_c = d_test[2]*256\n",
    "        y_c = d_test[3]*256\n",
    "\n",
    "        w = d_test[4]*256\n",
    "        h = d_test[5]*256\n",
    "\n",
    "        area = w*h\n",
    "        x_d = x_sldc + x_c - w/2\n",
    "        y_d = y_sldc + y_c - h/2\n",
    "\n",
    "        # checking negative corrdinates and null width or height\n",
    "        if x_c<0 or y_c<0:\n",
    "            count_neg_coord += 1\n",
    "            continue\n",
    "\n",
    "        elif int(w)==0 or int(h)==0:\n",
    "            count_side_zero += 1\n",
    "            continue\n",
    "        \n",
    "        # Condition on the area of the detection to suppress small/empty detections        \n",
    "        if area < 5000:\n",
    "            count_small_area += 1\n",
    "        \n",
    "        # If condition respected, construction of the tile, twice the size of the detection\n",
    "        if area >= 5000:\n",
    "            width_n = 2*w\n",
    "            height_n = 2*h\n",
    "            x_n = x_d - round((w/2))\n",
    "            y_n = y_d - round((h/2))\n",
    "    \n",
    "            if x_n < 0:\n",
    "                x_n = 0\n",
    "            if y_n < 0 :\n",
    "                y_n = 0\n",
    "    \n",
    "            detect_n = construct_tile(tile_builder, (int(x_n), int(y_n)), int(width_n), int(height_n))\n",
    "            detect_bw = rgb2gray(detect_n)\n",
    "            \n",
    "            # Tenenbaum gradient threshold\n",
    "            ten = tenenbaum(detect_bw)\n",
    "            \n",
    "            if ten >= 20:\n",
    "                save_path = os.path.join(detections_path, str(int(x_n)) + \"_\" + str(int(y_n)) +\"_from_\" + str(int(x_sldc)) + \"_\" + str(int(y_sldc)) + \".png\")\n",
    "                plt.imsave(save_path, detect_n)\n",
    "                count_saved_tiles += 1\n",
    "            else :\n",
    "                count_blur +=1\n",
    "\n",
    "\n",
    "\n",
    "    print(count_saved_tiles, \" saved detections and \", count_blur, \" images suppressed by the Tenenbaum gradient condition\")\n",
    "    \n",
    "    tot_blurr_tiles += count_blur\n",
    "    tot_saved_tiles += count_saved_tiles\n",
    "    tot_small_area += count_small_area\n",
    "    \n",
    "\n",
    "print('In total,', tot_saved_tiles, ' tiles were saved, ', tot_small_area,' were deleted because their area < 5000 and ', tot_blurr_tiles,' tiles with an area greater than 5000 were classified as fuzzy.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of the detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import save_img, img_to_array, array_to_img\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from collections import defaultdict\n",
    "from os.path import basename, join, exists, dirname, realpath\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from umap import UMAP, AlignedUMAP\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from urllib.parse import unquote # python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_filename(s, **kwargs):\n",
    "  '''Given a string that points to a filename, return a clean filename'''\n",
    "  s = unquote(os.path.basename(s))\n",
    "  invalid_chars = '<>:;,\"/\\\\|?*[]'\n",
    "  for i in invalid_chars: s = s.replace(i, '')\n",
    "  return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction of the Inception vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames = [filename for filename in os.listdir(detections_path) if filename.endswith(('.png', '.jpg'))]\n",
    "image_paths = [os.path.join(detections_path, filename) for filename in image_filenames]\n",
    "image_paths = sorted(image_paths)\n",
    "\n",
    "vector_dir = os.path.join(detections_path, 'inception_vectors')\n",
    "if not os.path.exists(vector_dir): os.makedirs(vector_dir)\n",
    "base = InceptionV3(include_top=True, weights='imagenet',)\n",
    "model = Model(inputs=base.input, outputs=base.get_layer('avg_pool').output)\n",
    "\n",
    "vecs = []\n",
    "nbr_wave = len(image_filenames)//100\n",
    "for j in range(nbr_wave+1):\n",
    "    with tqdm(100) as progress_bar:\n",
    "        for i in range(100):            \n",
    "            vector_path = os.path.join(vector_dir, clean_filename(image_paths[i+(j*100)]) + '.npy')\n",
    "            if os.path.exists(vector_path):\n",
    "                vec = np.load(vector_path)\n",
    "            else:\n",
    "                image = load_img(image_paths[i+(j*100)])\n",
    "                im = preprocess_input(img_to_array(image.resize((299,299))))\n",
    "                vec = model.predict(np.expand_dims(im, 0)).squeeze()\n",
    "                np.save(vector_path, vec)\n",
    "            vecs.append(vec)\n",
    "            progress_bar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA and UMAP application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = np.array(vecs)\n",
    "\n",
    "# PCA\n",
    "\n",
    "w_tot = PCA(n_components=min(100, len(vecs)), random_state= 24).fit_transform(vecs)\n",
    "print(\"PCA done\")\n",
    "\n",
    "# UMAP\n",
    "n_n = 30*(len(detections_path)/5016)\n",
    "\n",
    "model = UMAP(\n",
    "    n_neighbors = n_n,\n",
    "    min_dist = 0.0,\n",
    "    n_components = 2,\n",
    "    metric = 'correlation',\n",
    "    random_state = 24,\n",
    "    transform_seed = 24\n",
    ")\n",
    "\n",
    "print(\"UMAP reduction done\")\n",
    "\n",
    "z_tot = model.fit(w_tot).embedding_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_path = \"knn_classifier.joblib\"\n",
    "knn_load = joblib.load(classifier_path)\n",
    "\n",
    "n_labels = knn_load.predict(z_tot)\n",
    "\n",
    "interest_points = z_tot[n_labels == 0]\n",
    "\n",
    "\n",
    "# Visualiser les nouvelles données et le cluster d'intérêt\n",
    "# plt.scatter(z_tot[:, 0], z_tot[:, 1], c=n_labels, cmap='viridis', alpha=0.2)\n",
    "# plt.scatter(interest_points[:, 0], interest_points[:, 1], c='red', label='cluster of interest', alpha=0.2)\n",
    "# plt.xlabel('UMAP 1')\n",
    "# plt.ylabel('UMAP 2')\n",
    "# plt.legend()\n",
    "# plt.title('All data classification')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy cluster of interest in a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(image_paths))\n",
    "print(len(n_labels))\n",
    "interest_filenames = [image_paths[i] for i in range(len(n_labels)) if n_labels[i]==0]\n",
    "non_interest_filenames = [image_paths[i] for i in range(len(n_labels)) if n_labels[i]!= 0]\n",
    "\n",
    "print(len(interest_filenames))\n",
    "print(len(non_interest_filenames))\n",
    "\n",
    "interest_folder = 'cluster_of_interest'\n",
    "if not os.path.exists(interest_folder):\n",
    "    os.makedirs(interest_folder)\n",
    "\n",
    "for i in range(len(interest_filenames)):\n",
    "    filename = interest_filenames[i]\n",
    "    basename = os.path.basename(filename)\n",
    "    save_path = os.path.join(interest_folder, basename)\n",
    "    shutil.copy(filename, save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixplot Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixplot must also be installed\n",
    "\n",
    "https://github.com/YaleDHLab/pix-plot/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "command_pixplot = 'pixplot --images \"detections/*.png\" --n_neighbors 30 --min_dist 0.0'\n",
    "process_pixplot = subprocess.Popen(command1, shell=True)\n",
    "process_pixplot.wait() \n",
    "\n",
    "command2 = 'python -m http.server 5000'\n",
    "process2 = subprocess.Popen(command2, shell=True)\n",
    "process2.wait() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
